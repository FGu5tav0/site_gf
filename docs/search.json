[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gustavo Frosi",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n  \n     CV.Lattes\n  \n  \n     ResearchGate\n  \n\n\n\nOlá! Sou Gustavo Frosi, formado em engenharia agronômica e mestre em ciência do solo. Atualmente, estou cursando doutorado na mesma área, concentrando minhas pesquisas e interesses de estudo na química e mineralogia do solo. Durante meu mestrado, investigamos o impacto da aplicação de resíduo industrial nos parâmetros químicos e mineralógicos do solo. No doutorado, meu foco é estabelecer a relação entre a mineralogia do solo como componente fundamental para a adubação de potássio e a capacidade de prever, por meio da mineralogia, o comportamento do nutriente e sua dinâmica no solo.\nAo longo da minha trajetória acadêmica, estabeleci um contato constante com dados e análises estatísticas, o que, ao longo dos anos, conduziu ao desenvolvimento de um verdadeiro interesse por essa área. Inegavelmente, ciência e dados caminham lado a lado.\n\nA estatística é a gramática da ciência - Karl Pearson\n\nEsta frase de Pearson descreve bem meu apego e dedicação ao estudo da análise de dados e estatística, motivando-me a avançar nos estudos nessa área.\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "recap.html",
    "href": "recap.html",
    "title": "Coisas legais!",
    "section": "",
    "text": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos\n\n\n\nR\n\n\nRegressão Linear\n\n\nEstatística\n\n\n\nAplicando a tecnologia de regressão linear para dados contínuos e/ou discretos\n\n\n\nGustavo Frosi\n\n\n21 de ago. de 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantos artigos eu preciso ter no currículo até o fim do doutorado em Ciência do Solo?\n\n\n\nR\n\n\nDoutorado\n\n\nDataViz\n\n\nggplot2\n\n\n\nAproximação da quantidade de artigos que alunos possuem publicados ao final do Doutorado\n\n\n\nGustavo Frosi\n\n\n15 de abr. de 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBananaBudget\n\n\n\nR\n\n\nDataViz\n\n\nggplot2\n\n\n\nAnálise exploratória de gastos do mercado com o R com foco em visualização.\n\n\n\nGustavo Frosi, Diogo Bolzan\n\n\n15 de fev. de 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scraping Nota Fiscal Gaúcha\n\n\n\nR\n\n\npython\n\n\nWeb scraping\n\n\n\nAutomatizando o download de notas para consumidor final e a extração de dados.\n\n\n\nGustavo Frosi\n\n\n6 de fev. de 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrabalhos - XXIII CLACS | XXXVIII CBCS\n\n\n\nR\n\n\nSolos\n\n\n\nExplorando a relação dos trabalhos publicados e apresentados no XXIII CLACS | XXXVIII CBCS.\n\n\n\nGustavo Frosi, Dayana Eckert\n\n\n17 de dez. de 2023\n\n\n\n\n\n\n\n\nNenhum item correspondente\n\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "Projetos/index.html",
    "href": "Projetos/index.html",
    "title": "Gustavo Frosi",
    "section": "",
    "text": "Em construção…"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Gustavo Frosi",
    "section": "",
    "text": "Página em construção\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "recap/2023-12-17 CBCS 2023/index.html",
    "href": "recap/2023-12-17 CBCS 2023/index.html",
    "title": "Trabalhos - XXIII CLACS | XXXVIII CBCS",
    "section": "",
    "text": "O Congresso Brasileiro de Ciência do Solo (CBCS) é um evento técnico-científico que ocorre a cada dois anos (anos ímpares). O objetivo deste evento é reunir alunos, professores, pesquisadores e profissionais das áreas afins para a troca de conhecimento e discussão sobre as futuras perspectivas da ciência do solo.\nEsse ano o XXXVIII CBCS ocorreu em Florianopolis – SC, concomitante a ele ocorreu o XXIII Congresso Latino-Americado de Ciência do solo (CLACS). O evento em conjunto foi chamado de SOLOS FLORIPA 2023, com organização das sociedades Latino-americana (SLCS) e Brasileira (SBCS) da Ciência do Solo e realização da Empresa de Pesquisa Agropecuária e Extensão Rural de Santa Catarina (Epagri).\nUma das características mais importante do congresso é a possibilidade de os “pesquisadores” apresentarem trabalhos técnico-científicos sobre seus respectivos objetos de estudo. Diante disso, surge a dúvida em saber quais as áreas/subáreas teve as maiores quantidades de trabalhos apresentados e talvez entender o foco das pesquisas em Ciência do Solo na atualidade. Para sanar essa dúvida (pelo menos parcialmente) apresento nesse post uma forma de retirar os dados do site do congresso e realizar uma apresentação gráfica sobre os trabalhos. Para isso utilizo a linguagem de programação R.\nPara mais informações acesse o site: https://solosfloripa2023.com.br/solos2023"
  },
  {
    "objectID": "recap/2023-12-17 CBCS 2023/index.html#dados",
    "href": "recap/2023-12-17 CBCS 2023/index.html#dados",
    "title": "Trabalhos - XXIII CLACS | XXXVIII CBCS",
    "section": "Dados",
    "text": "Dados\nOs dados do número de trabalhos está na página de Trabalho Aprovados no site. A captura dos dados foi realizada com o endereço eletrônico da página. Um avaliação prévia de como se conportava a página e como os dados aparaciam foi realizada.\n\n\nCódigo\nprimeira_parte &lt;- \"https://solosfloripa2023.com.br/evento/solos2023/trabalhosaprovados?titulo=&autor=&t1area_id=\"\n\nid &lt;- c(1, 2, 3, 8, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16)\n\nsegunda_parte &lt;- \"&modeloformaapresentacaofinal_id=&btBuscar=Buscar\"\n\n\nUm loop foi realizado para remover os dados de cada uma das áreas e subareas.\n\n\nCódigo\ndf &lt;- as.data.frame(x = 1:length(id))\nfor (i in 1:16) {\n  dados &lt;- httr::GET(paste0(primeira_parte, id[i], segunda_parte))\n  get_content &lt;- httr::content(dados)\n\n  raw_table &lt;- get_content |&gt;\n    rvest::html_nodes(\".container-fluid\")\n\n  df[i, ] &lt;-\n    raw_table[[3]] |&gt;\n    rvest::html_text() |&gt;\n    strsplit(\"\\n\") |&gt;\n    unlist() |&gt;\n    enframe(name = NULL, value = \"linha\") |&gt;\n    dplyr::filter(stringr::str_detect(linha, pattern = \".*registro\"))\n}\n\n\nAlém de captar as iformações, foi realizado um processamento e manipulação para organizar os dados.\n\n\nCódigo\ndivi &lt;- raw_table[[3]] |&gt;\n  rvest::html_text() |&gt;\n  strsplit(\"\\n\") |&gt;\n  unlist() |&gt;\n  enframe(name = NULL, value = \"linha\") |&gt;\n  dplyr::filter(stringr::str_detect(linha, pattern = \" TODASDivisã\")) |&gt;\n  str_split(pattern = \"Di\", simplify = T) |&gt;\n  as.data.frame() |&gt;\n  pivot_longer(cols = 2:17, names_to = \"Divi\", values_to = \"nome\") |&gt;\n  filter(Divi != \"V1\") |&gt;\n  mutate(di = \"Di\") |&gt;\n  transmute(Divi = paste(di, nome, sep = \"\"))\n\n\nd_final &lt;- cbind(divi, df)\n\ndados.ok &lt;- d_final |&gt;\n  mutate(\n    partici = str_extract(`1:length(id)`,\n      pattern = \"[:digit:]{1,}\"\n    ),\n    partici = as.numeric(partici)\n  ) \n\n\ncomi &lt;- str_split(d_final$Divi, \":\")\ndata_separado &lt;- data.frame(do.call(rbind, comi))\n\ndf_ok &lt;- cbind(data_separado, dados.ok)\n\ndf_ok &lt;- df_ok |&gt; mutate(remover = str_sub(df_ok$X2, start = 16, end = 90))\n\nfont_add(\"Didot\", \"GFS_Didot/GFSDidot-Regular.ttf\")\n\nshowtext_auto()"
  },
  {
    "objectID": "recap/2023-12-17 CBCS 2023/index.html#trabalhos-dentro-de-cada-divisão",
    "href": "recap/2023-12-17 CBCS 2023/index.html#trabalhos-dentro-de-cada-divisão",
    "title": "Trabalhos - XXIII CLACS | XXXVIII CBCS",
    "section": "Trabalhos dentro de cada divisão",
    "text": "Trabalhos dentro de cada divisão\n\nDivisão 1Divisão 2Divisão 3Divisão 4\n\n\n\n\nCódigo\ndf_ok |&gt;\n  filter(X1 == \"Divisão 1 – Solo no espaço e no tempo\") |&gt;\n  ggplot(aes(y = partici, x = fct_reorder(remover, partici))) +\n  coord_flip() +\n  geom_col(fill = \"#ef8118\", col = \"#bc6a1d\") +\n  # facet_wrap(~X1, scales = \"free\") +\n  labs(\n    x = NULL, y = \"Nº de trabalhos\",\n    title = 'Resumos no &lt;span style = \"color:#ef8118\"&gt; XXIII CLACS | XXXVIII CBCS &lt;/span&gt;',\n    subtitle = \"Divisão 1 – Solo no espaço e no tempo\"\n  ) +\n  geom_text(aes(label = partici), nudge_y = -2, col = \"black\", size = 5) +\n  theme_minimal(24) +\n  theme(\n    text = element_text(family = \"Didot\"),\n    plot.title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\ndf_ok |&gt;\n  filter(X1 == \"Divisão 2 – Processos e Propriedades do Solo\") |&gt;\n  ggplot(aes(y = partici, x = fct_reorder(remover, partici))) +\n  coord_flip() +\n  geom_col(fill = \"#ef8118\", col = \"#bc6a1d\") +\n  # facet_wrap(~X1, scales = \"free\") +\n  labs(\n    x = NULL, y = \"Nº de trabalhos\",\n    title = 'Resumos no &lt;span style = \"color:#ef8118\"&gt; XXIII CLACS | XXXVIII CBCS &lt;/span&gt;',\n    subtitle = \"Divisão 2 – Processos e Propriedades do Solo\"\n  ) +\n  geom_text(aes(label = partici), nudge_y = -4, col = \"black\", size = 6) +\n  theme_minimal(24) +\n  theme(\n    text = element_text(family = \"Didot\"),\n     plot.title  = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\ndf_ok |&gt;\n  filter(X1 == \"Divisão 3 – Uso e Manejo do Solo\") |&gt; \nmutate(remover = factor(remover, levels = c(\n  \" Fertilidade do Solo e Nutrição de Plantas\",\n  \" Corretivos e Fertilizantes\", \n  \" Manejo e Conservação do Solo e da Água\", \n  \" Planejamento do Uso da Terra\", \n  \" Poluição, Remediação do Solo e Recuperação de Áreas Degradadas\"\n), labels = c(\n  \"Fertilidade do Solo e \\n Nutrição de Plantas\",\n  \"Corretivos e Fertilizantes\",\n  \"Manejo e Conservação \\n do Solo e da Água\",\n  \"Planejamento do Uso da Terra\",\n  \"Poluição, Remediação do Solo e \\n Recuperação de Áreas Degradadas\"\n))) |&gt;\n  ggplot(aes(y = partici, x = fct_reorder(remover, partici))) +\n  coord_flip() +\n  geom_col(fill = \"#ef8118\", col = \"#bc6a1d\") +\n  # facet_wrap(~X1, scales = \"free\") +\n  labs(\n    x = NULL, y = \"Nº de trabalhos\",\n    title = 'Resumos no &lt;span style = \"color:#ef8118\"&gt; XXIII CLACS | XXXVIII CBCS &lt;/span&gt;',\n    subtitle = \"Divisão 3 – Uso e Manejo do Solo\"\n  ) +\n  geom_text(aes(label = partici), nudge_y = -13, col = \"black\", size = 6) +\n  theme_minimal(24) +\n  theme(\n    text = element_text(family = \"Didot\"),\n    plot.title = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCódigo\ndf_ok |&gt;\n  filter(X1 == \"Divisão 4 - Solo, Ambiente e Sociedade\") |&gt;\n  mutate(remover = factor(remover, labels = c(\n    \"Educação em Solos e \\n Percepção Pública do Solo\",\n    \"História, Epistemologia e \\n Sociologia da Ciência\",\n    \"Solos e Segurança Alimentar\"\n  ))) |&gt;\n  ggplot(aes(y = partici, x = fct_reorder(remover, partici))) +\n  coord_flip() +\n  geom_col(fill = \"#ef8118\", col = \"#bc6a1d\") +\n  # facet_wrap(~X1, scales = \"free\") +\n  labs(\n    x = NULL, y = \"Nº de trabalhos\",\n    title = 'Resumos no &lt;span style = \"color:#ef8118\"&gt; XXIII CLACS | XXXVIII CBCS &lt;/span&gt;',\n    subtitle = \"Divisão 4 - Solo, Ambiente e Sociedade\"\n  ) +\n  geom_text(aes(label = partici), nudge_y = -2, col = \"black\", size = 6) +\n  theme_minimal(24) +\n  theme(\n    text = element_text(family = \"Didot\"),\n     plot.title  = element_markdown()\n  )"
  },
  {
    "objectID": "recap/2023-12-17 CBCS 2023/index.html#trabalhos-entre-as-divisões",
    "href": "recap/2023-12-17 CBCS 2023/index.html#trabalhos-entre-as-divisões",
    "title": "Trabalhos - XXIII CLACS | XXXVIII CBCS",
    "section": "Trabalhos entre as divisões",
    "text": "Trabalhos entre as divisões\nDe forma geral, observa-se que a maioria dos trabalhos submetidos e apresentados no evento pertence à divisão científica 3 – Uso e Manejo do Solo, totalizando 829 trabalhos. Em contrapartida, a divisão 4 – Solo, Ambiente e Sociedade apresentou o menor número de trabalhos, com apenas 58.\nNão podemos afirmar que esses números representam a totalidade dos trabalhos realizados em cada área, mas eles podem fornecer uma amostra do cenário atual das pesquisas em Ciência do Solo.\nA disparidade nos estudos entre as áreas da Ciência do Solo levanta questionamentos sobre a direção da pesquisa nesse campo. É pertinente refletir se nossas investigações estão se aproximando das necessidades da sociedade e se estão verdadeiramente cumprindo o papel que a Ciência deve desempenhar.\nEssa discrepância nos números sugere a importância de uma análise mais aprofundada sobre os temas predominantes e as lacunas existentes na pesquisa em Ciência do Solo. Incentivar a discussão dentro da comunidade científica pode ser valioso para garantir que as pesquisas estejam alinhadas com as demandas sociais e ambientais atuais.\n\n\nCódigo\ndf_ok |&gt;\n  group_by(X1) |&gt;\n  summarise(soma = sum(partici)) |&gt;\n  mutate(\n    X1 = factor(X1, labels = c(\n      \"Divisão 1 – Solo no \\n espaço e no tempo\",\n      \"Divisão 2 – Processos e \\n  Propriedades do Solo\",\n      \"Divisão 3 – Uso e \\n Manejo do Solo\",\n      \"Divisão 4 - Solo, \\n Ambiente e Sociedade\"\n    )),\n    total = sum(soma)\n  ) |&gt;\n  ggplot(aes(y = soma, x = fct_reorder(X1, soma))) +\n  coord_flip() +\n  geom_col(fill = \"#ef8118\", col = \"#bc6a1d\") +\n  # facet_wrap(~X1, scales = \"free\") +\n  labs(\n    x = NULL, y = \"Nº de trabalhos\",\n    title = 'Resumos no &lt;span style = \"color:#ef8118\"&gt; XXIII CLACS | XXXVIII CBCS &lt;/span&gt;',\n    subtitle = \"Total por divisão\"\n  ) +\n  geom_text(aes(label = soma), nudge_y = -32, col = \"black\", size = 6) +\n  theme_minimal(24) +\n  theme(\n    text = element_text(family = \"Didot\"),\n    plot.title = element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html",
    "href": "recap/2024-02-05 webscraping python/index.html",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "",
    "text": "Você já se perguntou quanto gastou no mercado? E quais os itens que mais comprou? Bom, se você disse que não, acho melhor começar a pensar, e se você disse que sim, mas não sabe como fazer para buscar esses dados, eu tenho uma boa notícia!\nNeste texto, descrevo como utilizei o Python na criação de um código que automatizou o download das minhas notas fiscais pessoais (NFC: Nota Fiscal de Consumidor). Deixo claro que o código é destinado ao uso pessoal e doméstico. Minha intenção é apenas o controle de gastos financeiros (e outras brincadeiras com dados).\nO exemplo que utilizo aqui é para o site Nota Fiscal Gaúcha e serve apenas para quem utiliza o “benefício”. Fica obvio que é necessário colocar o CPF em todas as compras né.\nAntes de começar a codificar, é necessário baixar o webdriver, uma ferramenta para testes de automação que oferece uma série de recursos para utilizar o navegador. O webdriver a ser baixado depende do navegador que você utiliza e da versão instalada. No meu caso, vou ensinar com o navegador da Google, o Google Chrome na versão 121.0.6167.140.\nPara fazer o download do webdriver, basta acessar o link\nDepois de baixado, é necessário colar o webdriver na pasta onde está o executável do Python. No meu caso, como utilizo o Anaconda, colei no diretório do Anaconda, o mesmo onde fica o Python.\nAgora sim! Com isso, já é possível começar a escrever códigos."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#manipulação-de-banco-baixado",
    "href": "recap/2024-02-05 webscraping python/index.html#manipulação-de-banco-baixado",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Manipulação de banco baixado",
    "text": "Manipulação de banco baixado\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\n\nCódigo\nlibrary(tidyverse)\n\n\ndados &lt;- readxl::read_excel(\"notas/Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\n\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")\n\n\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#ajustando-a-planilha-com-a-chave-de-acesso",
    "href": "recap/2024-02-05 webscraping python/index.html#ajustando-a-planilha-com-a-chave-de-acesso",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Ajustando a planilha com a chave de acesso",
    "text": "Ajustando a planilha com a chave de acesso\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\nlibrary(tidyverse)\n\n\ndados &lt;- readxl::read_excel(\"Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")\n\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#ajuste-da-chave-de-acesso",
    "href": "recap/2024-02-05 webscraping python/index.html#ajuste-da-chave-de-acesso",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Ajuste da chave de acesso",
    "text": "Ajuste da chave de acesso\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\n\n\n\n\n\nNota\n\n\n\nCódigos em R\n\n\n\nlibrary(tidyverse)\n\ndados &lt;- readxl::read_excel(\"Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")"
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-click",
    "href": "recap/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-click",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Encontrando os pontos de click",
    "text": "Encontrando os pontos de click\n\n\n\n\n\n\nNota\n\n\n\nAgora serão apenas códigos em python\n\n\nO procedimento que apresento aqui é baseado em um algoritmo que controla o mouse do computador, realizando cliques e movimentos programados. Como é de se esperar, é necessário fornecer as coordenadas para guiar os movimentos. Essa é uma tarefa manual, porém, é realizada apenas uma vez e se aplica a todas as notas (baixei mais de 200 notas).\nUtilizei o MouseInfo para identificar os pontos. Após abrir o console do python (melhor fazer pelo console) é só dar os seguintes comandos:\n\nfrom mouseinfo import mouseInfo\n\nmouseInfo()\n\nBasta realizar um teste com o MouseInfo aberto. Abra o site onde as notas serão baixadas e identifique os pontos onde será necessário realizar os cliques.\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-clique",
    "href": "recap/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-clique",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Encontrando os pontos de clique",
    "text": "Encontrando os pontos de clique\n\n\n\n\n\n\nNota\n\n\n\nAgora serão apenas códigos em python\n\n\nO procedimento que apresento aqui é baseado em um algoritmo que controla o mouse do computador, realizando cliques e movimentos programados. Como é de se esperar, é necessário fornecer as coordenadas para guiar os movimentos. Essa é uma tarefa manual, porém, é realizada apenas uma vez e se aplica a todas as notas (baixei mais de 200 notas).\nUtilizei o MouseInfo para identificar os pontos. Após abrir o console do python (melhor fazer pelo console) é só dar os seguintes comandos:\n\nfrom mouseinfo import mouseInfo\n\nmouseInfo()\n\nO vídeo abaixo demonstra o funcionamento do MouseInfo, exibindo as marcações das coordenadas x e y à medida que o mouse é movido.\nVideo\nPara encontar os ponto certo basta realizar um teste com o MouseInfo aberto. Abra o site onde as notas serão baixadas e identifique os pontos onde será necessário realizar os cliques. O atalho F6 pode ser utilizado no MouseInfo para marcar as pontos x e y. Siga os passo:\n1º Passo: Clique em avançar e depois em imprimir Figura 2\n\n\n\n\n\n\nFigura 2\n\n\n\n2º Passo: Clique em imprimir na parte do pdf Figura 3\n\n\n\n\n\n\nFigura 3\n\n\n\n3º Passo: Clique em salvar Figura 4\n\n\n\n\n\n\nFigura 4\n\n\n\nPor padrão, o Windows salva na pasta de downloads, mas é possível alterar para a pasta desejada. Basta encontrar o ponto de clique e realizar a mudança de destino."
  },
  {
    "objectID": "recap/2024-02-05 webscraping python/index.html#resultado",
    "href": "recap/2024-02-05 webscraping python/index.html#resultado",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Resultado",
    "text": "Resultado\nO vídeo exibe o “robô” em ação, realizando a impressão e salvando automaticamente cada uma das notas. Na minha análise, as 205 notas que obtive foram processadas em quase 21 minutos, o que equivale a ~6 segundos por nota (Um computador melhor fará em menos tempo). E você, ocupado com diversas tarefas, conseguiria realizar isso de maneira mais rápida? Mesmo ao atingir a nota de número 100 e perceber que ainda falta mais da metade. Se acha que não, a resposta para isso é simples: programação!\nVideo\nE agora, o que fazer com essas notas? Começar a extrair os dados manualmente? Pagar alguém para fazer? Excluir do computador e dar um ponto final?\nA resposta óbvia é sim, dar um ponto final e excluir. No entanto, se ainda assim você deseja extrair informações dos seus dados, eu aconselho a conferir um próximo post (no futuro), no qual explico o que fazer com as notas.\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "projetos.html",
    "href": "projetos.html",
    "title": "Projetos",
    "section": "",
    "text": "Aqui você encontra alguns dos meus projetos.\n\n\n\n\n\n\n\n\n\n\n\nBananaBudget\n\n\n\n\n\nProjeto de visualização dos gastos mensais no mercado\n\n\n\n\n\n1 de nov. de 2023\n\n\nGustavo Frosi, Diogo Vieri\n\n\n\n\n\n\n\n\n\n\n\n\nR para ciência do solo\n\n\n\n\n\nDisciplina de introdução à linguagem de programação R para estudos em Ciência do Solo - UFRGS\n\n\n\n\n\nGustavo Frosi, Gustavo Pesini\n\n\n\n\n\n\nNenhum item correspondente\n\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "projetos/2024-02-05 webscraping python/index.html",
    "href": "projetos/2024-02-05 webscraping python/index.html",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "",
    "text": "Você já se perguntou quanto gastou no mercado? E quais os itens que mais comprou? Bom, se você disse que não, acho melhor começar a pensar, e se você disse que sim, mas não sabe como fazer para buscar esses dados, eu tenho uma boa notícia!\nNeste texto, descrevo como utilizei o Python na criação de um código que automatizou o download das minhas notas fiscais pessoais (NFC: Nota Fiscal de Consumidor). Deixo claro que o código é destinado ao uso pessoal e doméstico. Minha intenção é apenas o controle de gastos financeiros (e outras brincadeiras com dados).\nO exemplo que utilizo aqui é para o site Nota Fiscal Gaúcha e serve apenas para quem utiliza o “benefício”. Fica obvio que é necessário colocar o CPF em todas as compras né.\nAntes de começar a codificar, é necessário baixar o webdriver, uma ferramenta para testes de automação que oferece uma série de recursos para utilizar o navegador. O webdriver a ser baixado depende do navegador que você utiliza e da versão instalada. No meu caso, vou ensinar com o navegador da Google, o Google Chrome na versão 121.0.6167.140.\nPara fazer o download do webdriver, basta acessar o link\nDepois de baixado, é necessário colar o webdriver na pasta onde está o executável do Python. No meu caso, como utilizo o Anaconda, colei no diretório do Anaconda, o mesmo onde fica o Python.\nAgora sim! Com isso, já é possível começar a escrever códigos."
  },
  {
    "objectID": "projetos/2024-02-05 webscraping python/index.html#ajuste-da-chave-de-acesso",
    "href": "projetos/2024-02-05 webscraping python/index.html#ajuste-da-chave-de-acesso",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Ajuste da chave de acesso",
    "text": "Ajuste da chave de acesso\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\n\n\n\n\n\nNota\n\n\n\nCódigos em R\n\n\n\nlibrary(tidyverse)\n\ndados &lt;- readxl::read_excel(\"Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")"
  },
  {
    "objectID": "projetos/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-clique",
    "href": "projetos/2024-02-05 webscraping python/index.html#encontrando-os-pontos-de-clique",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Encontrando os pontos de clique",
    "text": "Encontrando os pontos de clique\n\n\n\n\n\n\nNota\n\n\n\nAgora serão apenas códigos em python\n\n\nO procedimento que apresento aqui é baseado em um algoritmo que controla o mouse do computador, realizando cliques e movimentos programados. Como é de se esperar, é necessário fornecer as coordenadas para guiar os movimentos. Essa é uma tarefa manual, porém, é realizada apenas uma vez e se aplica a todas as notas (baixei mais de 200 notas).\nUtilizei o MouseInfo para identificar os pontos. Após abrir o console do python (melhor fazer pelo console) é só dar os seguintes comandos:\n\nfrom mouseinfo import mouseInfo\n\nmouseInfo()\n\nO vídeo abaixo demonstra o funcionamento do MouseInfo, exibindo as marcações das coordenadas x e y à medida que o mouse é movido.\nVideo\nPara encontar os ponto certo basta realizar um teste com o MouseInfo aberto. Abra o site onde as notas serão baixadas e identifique os pontos onde será necessário realizar os cliques. O atalho F6 pode ser utilizado no MouseInfo para marcar as pontos x e y. Siga os passo:\n1º Passo: Clique em avançar e depois em imprimir Figura 2\n\n\n\n\n\n\nFigura 2\n\n\n\n2º Passo: Clique em imprimir na parte do pdf Figura 3\n\n\n\n\n\n\nFigura 3\n\n\n\n3º Passo: Clique em salvar Figura 4\n\n\n\n\n\n\nFigura 4\n\n\n\nPor padrão, o Windows salva na pasta de downloads, mas é possível alterar para a pasta desejada. Basta encontrar o ponto de clique e realizar a mudança de destino."
  },
  {
    "objectID": "projetos/2024-02-05 webscraping python/index.html#resultado",
    "href": "projetos/2024-02-05 webscraping python/index.html#resultado",
    "title": "Web scraping Nota Fiscal Gaúcha",
    "section": "Resultado",
    "text": "Resultado\nO vídeo exibe o “robô” em ação, realizando a impressão e salvando automaticamente cada uma das notas. Na minha análise, as 205 notas que obtive foram processadas em quase 21 minutos, o que equivale a ~6 segundos por nota (Um computador melhor fará em menos tempo). E você, ocupado com diversas tarefas, conseguiria realizar isso de maneira mais rápida? Mesmo ao atingir a nota de número 100 e perceber que ainda falta mais da metade. Se acha que não, a resposta para isso é simples: programação!\nVideo\nE agora, o que fazer com essas notas? Começar a extrair os dados manualmente? Pagar alguém para fazer? Excluir do computador e dar um ponto final?\nA resposta óbvia é sim, dar um ponto final e excluir. No entanto, se ainda assim você deseja extrair informações dos seus dados, eu aconselho a conferir um próximo post (no futuro), no qual explico o que fazer com as notas.\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html",
    "title": "R para ciência do solo",
    "section": "",
    "text": "Disciplina de R\nNo ano de 2023, um importante passo foi dado na Pós-Graduação em Ciência do Solo da UFRGS: a criação da primeira disciplina com foco total no ensino e aprendizado da linguagem de programação R. O mais entusiasmante foi poder contribuir com todo esse processo.\nPor iniciativa do professor Tales Tiecher, eu, Gustavo Frosi, junto com Gustavo Pesini, realizamos a construção de uma série de aulas e exercícios práticos, com foco na manipulação de dados, análise e visualização desses dados, principalmente em estudos na Ciência do Solo. O principal objetivo da disciplina foi capacitar os alunos a desenvolverem suas próprias análises de dados utilizando os recursos da linguagem de programação R.\n\n\nCronograma\nUm total de 10 aulas foram produzidas (Tabela 1) e ministradas ao longo de pouco mais de um mês. O conteúdo programado começou desde a base, considerando que os alunos nunca tiveram contato com a ferramenta. Assim, iniciamos com uma ambientação, ensinando os conceitos mais básicos do funcionamento de uma linguagem de programação. Na segunda etapa, focamos na análise de dados, com procedimentos de manipulação e visualização de dados, além do uso dos principais testes estatísticos. A última etapa consistiu em uma breve explanação sobre análise multivariada e criação de relatórios com o R.\n\n\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#ajuste-da-chave-de-acesso",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#ajuste-da-chave-de-acesso",
    "title": "R para ciência do solo",
    "section": "Ajuste da chave de acesso",
    "text": "Ajuste da chave de acesso\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\n\n\n\n\n\nNota\n\n\n\nCódigos em R\n\n\n\nlibrary(tidyverse)\n\ndados &lt;- readxl::read_excel(\"Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")"
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#encontrando-os-pontos-de-clique",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#encontrando-os-pontos-de-clique",
    "title": "R para ciência do solo",
    "section": "Encontrando os pontos de clique",
    "text": "Encontrando os pontos de clique\n\n\n\n\n\n\nNota\n\n\n\nAgora serão apenas códigos em python\n\n\nO procedimento que apresento aqui é baseado em um algoritmo que controla o mouse do computador, realizando cliques e movimentos programados. Como é de se esperar, é necessário fornecer as coordenadas para guiar os movimentos. Essa é uma tarefa manual, porém, é realizada apenas uma vez e se aplica a todas as notas (baixei mais de 200 notas).\nUtilizei o MouseInfo para identificar os pontos. Após abrir o console do python (melhor fazer pelo console) é só dar os seguintes comandos:\n\nfrom mouseinfo import mouseInfo\n\nmouseInfo()\n\nO vídeo abaixo demonstra o funcionamento do MouseInfo, exibindo as marcações das coordenadas x e y à medida que o mouse é movido.\nVideo\nPara encontar os ponto certo basta realizar um teste com o MouseInfo aberto. Abra o site onde as notas serão baixadas e identifique os pontos onde será necessário realizar os cliques. O atalho F6 pode ser utilizado no MouseInfo para marcar as pontos x e y. Siga os passo:\n1º Passo: Clique em avançar e depois em imprimir Figura 2\n\n\n\n\n\n\nFigura 2\n\n\n\n2º Passo: Clique em imprimir na parte do pdf Figura 3\n\n\n\n\n\n\nFigura 3\n\n\n\n3º Passo: Clique em salvar Figura 4\n\n\n\n\n\n\nFigura 4\n\n\n\nPor padrão, o Windows salva na pasta de downloads, mas é possível alterar para a pasta desejada. Basta encontrar o ponto de clique e realizar a mudança de destino."
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#resultado",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#resultado",
    "title": "R para ciência do solo",
    "section": "Resultado",
    "text": "Resultado\nO vídeo exibe o “robô” em ação, realizando a impressão e salvando automaticamente cada uma das notas. Na minha análise, as 205 notas que obtive foram processadas em quase 21 minutos, o que equivale a ~6 segundos por nota (Um computador melhor fará em menos tempo). E você, ocupado com diversas tarefas, conseguiria realizar isso de maneira mais rápida? Mesmo ao atingir a nota de número 100 e perceber que ainda falta mais da metade. Se acha que não, a resposta para isso é simples: programação!\nVideo\nE agora, o que fazer com essas notas? Começar a extrair os dados manualmente? Pagar alguém para fazer? Excluir do computador e dar um ponto final?\nA resposta óbvia é sim, dar um ponto final e excluir. No entanto, se ainda assim você deseja extrair informações dos seus dados, eu aconselho a conferir um próximo post (no futuro), no qual explico o que fazer com as notas.\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "projetos/2023-11-01 bananabudget/index.html",
    "href": "projetos/2023-11-01 bananabudget/index.html",
    "title": "BananaBudget",
    "section": "",
    "text": "Você já se perguntou quanto gastou no mercado? E quais os itens que mais comprou? Bom, se você disse que não, acho melhor começar a pensar, e se você disse que sim, mas não sabe como fazer para buscar esses dados, eu tenho uma boa notícia!\nNeste texto, descrevo como utilizei o Python na criação de um código que automatizou o download das minhas notas fiscais pessoais (NFC: Nota Fiscal de Consumidor). Deixo claro que o código é destinado ao uso pessoal e doméstico. Minha intenção é apenas o controle de gastos financeiros (e outras brincadeiras com dados).\nO exemplo que utilizo aqui é para o site Nota Fiscal Gaúcha e serve apenas para quem utiliza o “benefício”. Fica obvio que é necessário colocar o CPF em todas as compras né.\nAntes de começar a codificar, é necessário baixar o webdriver, uma ferramenta para testes de automação que oferece uma série de recursos para utilizar o navegador. O webdriver a ser baixado depende do navegador que você utiliza e da versão instalada. No meu caso, vou ensinar com o navegador da Google, o Google Chrome na versão 121.0.6167.140.\nPara fazer o download do webdriver, basta acessar o link\nDepois de baixado, é necessário colar o webdriver na pasta onde está o executável do Python. No meu caso, como utilizo o Anaconda, colei no diretório do Anaconda, o mesmo onde fica o Python.\nAgora sim! Com isso, já é possível começar a escrever códigos."
  },
  {
    "objectID": "projetos/2023-11-01 bananabudget/index.html#ajuste-da-chave-de-acesso",
    "href": "projetos/2023-11-01 bananabudget/index.html#ajuste-da-chave-de-acesso",
    "title": "BananaBudget",
    "section": "Ajuste da chave de acesso",
    "text": "Ajuste da chave de acesso\nDa planilha baixada do site, apenas a coluna que contém a chave de acesso da nota fiscal é relevante. Realizei uma pequena manipulação na planilha, removendo os espaços da coluna da chave de acesso, e a salvei como arquivo CSV.\n\n\n\n\n\n\nNota\n\n\n\nCódigos em R\n\n\n\nlibrary(tidyverse)\n\ndados &lt;- readxl::read_excel(\"Nota Fiscal Gaúcha.xlsx\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(chave = chave_de_acesso |&gt; str_remove_all(pattern = \"\\\\s\")) |&gt;\n  select(chave)\n\nwrite.csv(x = dados, file = \"notas/para_auto.csv\")"
  },
  {
    "objectID": "projetos/2023-11-01 bananabudget/index.html#encontrando-os-pontos-de-clique",
    "href": "projetos/2023-11-01 bananabudget/index.html#encontrando-os-pontos-de-clique",
    "title": "BananaBudget",
    "section": "Encontrando os pontos de clique",
    "text": "Encontrando os pontos de clique\n\n\n\n\n\n\nNota\n\n\n\nAgora serão apenas códigos em python\n\n\nO procedimento que apresento aqui é baseado em um algoritmo que controla o mouse do computador, realizando cliques e movimentos programados. Como é de se esperar, é necessário fornecer as coordenadas para guiar os movimentos. Essa é uma tarefa manual, porém, é realizada apenas uma vez e se aplica a todas as notas (baixei mais de 200 notas).\nUtilizei o MouseInfo para identificar os pontos. Após abrir o console do python (melhor fazer pelo console) é só dar os seguintes comandos:\n\nfrom mouseinfo import mouseInfo\n\nmouseInfo()\n\nO vídeo abaixo demonstra o funcionamento do MouseInfo, exibindo as marcações das coordenadas x e y à medida que o mouse é movido.\nVideo\nPara encontar os ponto certo basta realizar um teste com o MouseInfo aberto. Abra o site onde as notas serão baixadas e identifique os pontos onde será necessário realizar os cliques. O atalho F6 pode ser utilizado no MouseInfo para marcar as pontos x e y. Siga os passo:\n1º Passo: Clique em avançar e depois em imprimir Figura 2\n\n\n\n\n\n\n\n\n\n2º Passo: Clique em imprimir na parte do pdf Figura 3\n\n\n\n\n\n\n\n\n\n3º Passo: Clique em salvar Figura 4\n\n\n\n\n\n\n\n\n\nPor padrão, o Windows salva na pasta de downloads, mas é possível alterar para a pasta desejada. Basta encontrar o ponto de clique e realizar a mudança de destino."
  },
  {
    "objectID": "projetos/2023-11-01 bananabudget/index.html#resultado",
    "href": "projetos/2023-11-01 bananabudget/index.html#resultado",
    "title": "BananaBudget",
    "section": "Resultado",
    "text": "Resultado\nO vídeo exibe o “robô” em ação, realizando a impressão e salvando automaticamente cada uma das notas. Na minha análise, as 205 notas que obtive foram processadas em quase 21 minutos, o que equivale a ~6 segundos por nota (Um computador melhor fará em menos tempo). E você, ocupado com diversas tarefas, conseguiria realizar isso de maneira mais rápida? Mesmo ao atingir a nota de número 100 e perceber que ainda falta mais da metade. Se acha que não, a resposta para isso é simples: programação!\nVideo\nE agora, o que fazer com essas notas? Começar a extrair os dados manualmente? Pagar alguém para fazer? Excluir do computador e dar um ponto final?\nA resposta óbvia é sim, dar um ponto final e excluir. No entanto, se ainda assim você deseja extrair informações dos seus dados, eu aconselho a conferir um próximo post (no futuro), no qual explico o que fazer com as notas.\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas."
  },
  {
    "objectID": "recap/2024-02-15 BananBudget1/index.html",
    "href": "recap/2024-02-15 BananBudget1/index.html",
    "title": "BananaBudget",
    "section": "",
    "text": "Introdução\n\n\nVocê já pensou qual seu gasto no mercado? Quais os produtos que mais compra? Qual a frequência que vai ao mercado?\nEu e o Diogo já!\n\n\n\n\n\n\n\n\nNeste texto, vamos mostrar como manipulamos notas fiscais de mercado para responder às questões iniciais por meio da programação. Morando em Porto Alegre com mais duas pessoas (totalizando quatro), é fundamental ter um controle de gastos. Após vários meses registrando todos os gastos no mercado, surgiu a seguinte pergunta: “Será que podemos identificar quais produtos compramos com maior ou menor frequência?” (coisa de doido, né?). De certa forma, já sabíamos que, ao utilizar o CPF nas compras, as notas ficam armazenadas em algum lugar, mas não tínhamos certeza de onde.\nNo Rio Grande do Sul, assim como em outros estados, existe um sistema de “recompensa” chamado Nota Fiscal Gaúcha. Ao adicionar o CPF nas notas, você concorre a prêmios e outras vantagens. Além disso, é possível acessar todas as notas fiscais, com informações como data, produtos adquiridos, preços e estabelecimentos.\nUma maneira de obter essas notas é por meio de “web scraping”. Para saber mais sobre esse processo, você pode conferir o Post, onde descrevemos como realizá-lo.\nCom notas na mão (ou no pc) é hora de organizar os dados!\n\n\nExtração de dados dos pdfs\nCriamos uma função chamada “pega_produto()”, essa função identifica padrões no texto e separa as informações em um data frame, onde cada linha representa um produto comprado. Ao aplicar essa função em todas as notas, obtemos uma base de dados com os seguintes campos: Data, Hora, Nome do Produto, Preço do Produto… (Figura 1)\n\n# Carregando os pacotes\nlibrary(pdftools) # manipulação de pdf\nlibrary(tidyverse) # conjunto de pacotes para manipulação e visualização\n\n# Primeiro defini o diretório da pasta com as notas.\npasta &lt;- \"notas/notas/\"\n\n# lista os arquivos dentro da pasta\ndocs &lt;- list.files(pasta, full.names = T)\n\n# a função 'pega_produto' será responsável por encontrar cada linha da nota com os produtos\npega_produto &lt;- function(pdf) {\n  df &lt;-\n    pdf_text(pdf) |&gt;\n    strsplit(\"\\n\") |&gt;\n    unlist() |&gt;\n    enframe(name = NULL, value = \"linha\")\n\n  data &lt;- df |&gt;\n    filter(str_detect(linha, pattern = \"Data de Emissão\")) |&gt;\n    str_extract(pattern = \"\\\\d{2}.\\\\d{2}.\\\\d{4}.\\\\d{2}.\\\\d{2}.\\\\d{2}\")\n\n  produtos &lt;- df |&gt;\n    filter(str_detect(linha, pattern = \"\\\\S\\\\d{4,}\")) |&gt;\n    filter(str_detect(linha, pattern = \"^\\\\s{4,}\\\\d\")) |&gt;\n    mutate(linha = linha |&gt; stringr::str_replace_all(pattern = \"[\\\\s]{3,}\", replacement = \"---\")) |&gt;\n    separate(\n      col = linha, into = c(\n        \"ts\", \"id\", \"nome\", \"quant\",\n        \"unidade\", \"valor_uni\", \"valor_total\"\n      ),\n      sep = \"---\", convert = F\n    ) |&gt;\n    select(-1)\n\n  dados_fim &lt;- mutate(\n    .data = produtos,\n    data = rep(\n      data,\n      length(produtos$id)\n    )\n  ) |&gt;\n    separate(col = data, into = c(\"data\", \"hora\"), sep = \" \") |&gt;\n    mutate(\n      hora = hora |&gt; str_extract(pattern = \"\\\\d{2}\\\\:\\\\d{2}\\\\:\\\\d{2}\"),\n      valor_uni = valor_uni |&gt; stringr::str_replace_all(\n        pattern = \"[\\\\,]\",\n        replacement = \".\"\n      ),\n      valor_total = valor_total |&gt; stringr::str_replace_all(\n        pattern = \"[\\\\,]\",\n        replacement = \".\"\n      )\n    ) |&gt;\n    relocate(data, hora) |&gt;\n    mutate(\n      data = dmy(data),\n      id = as.factor(id),\n      valor_uni = as.numeric(valor_uni),\n      valor_total = as.numeric(valor_total)\n    )\n\n  return(dados_fim)\n}\n\n# criando a base de dados\nbase &lt;- data.frame(data = NULL)\n\n# para utilizar a função pega_produto() em cada nota usamos um loop for e juntamos as linhas com a função bind_rows()\nfor (i in 1:length(docs)) {\n  b1 &lt;- pega_produto(docs[i])\n  base &lt;- base |&gt; bind_rows(b1)\n}\n\n# aqui visualizamos a base\nbase |&gt; glimpse()\n\n# e por fim salvamos\nsave(base, file = \"notas/base.RData\")\n\n\n\n\n\n\n\nFigura 1: base de dados\n\n\n\n\n\nOrganização dos dados\nNossa base de dados foi construída com 205 notas fiscais do período de janeiro a dezembro de 2023. Após uma visualização prévia, decidimos criar uma variável relacionada ao tipo de produto, resultando na variável “categoria” (conforme apresentado na Tabela 1). O número de categorias foi determinado subjetivamente, com a intenção de discriminar adequadamente os produtos.\n\n\n\n\n\nCategoria\nItem\n\n\n\n\nCarboidratos\nPão, Massas, Arroz…\n\n\nDoces\nChocolate, Sorvete, Bolacha…\n\n\nBebidas\nLeite, Refrigerante, Água…\n\n\nEmbutidos\nQueijo, Linguiça Calabresa, Bacon…\n\n\nCarnes E Ovos\nOvo, Carnes Vermelhas, Peito de Frango…\n\n\nHigiene\nDesodorante, Sabonete…\n\n\nFrutas\nBanana, Maça, Uva…\n\n\nVerduras\nTomate, Pepino, Alface…\n\n\nLimpeza\nAmaciante, Detergente…\n\n\nTemperos E Molhos\nMassa de Tomate, Salsa, Orégano…\n\n\n\n\n\nTivemos que tomar algumas decisões em relação aos produtos que iríamos incluir na tabela. O primeiro ponto foi determinar que produtos que tivessem apenas uma compra seriam excluídos. Nossa pergunta inicial não visa produtos específicos, mas sim um padrão de consumo geral.\nUma maneira que encontramos para agrupar produtos foi utilizar apenas a primeira palavra do nome do item. No entanto, observamos dois problemas: o primeiro foi a ocorrência de produtos iguais com nomes iniciais diferentes, enquanto o segundo foi a presença de produtos diferentes com nomes iniciais iguais.\nExemplo 1: A palavra \\(\\textbf{CHOC}\\) e \\(\\textbf{CHO}\\) representava o mesmo produto, chocolate.\nExemplo 2: A palavra \\(\\textbf{MINI}\\) era observada no \\(\\textbf{MINI TOMATE}\\) e no \\(\\textbf{MINI PANETONE}\\).\nEsses casos foram tratados de forma manual e caso a caso.\n\n# realizamos uma organização após uma visualização prêvia das categorias \nbase &lt;- base |&gt; \n  mutate(\n    categoria = as.factor(cat),\n    hora = hms(hora),\n    mes = month(data),\n    inicial = ini\n  ) |&gt;\n  select(!c(cat, ini)) |&gt;\n  mutate(\n    mes = case_match(\n      mes,\n      1 ~ \"jan\",\n      2 ~ \"fev\",\n      3 ~ \"mar\",\n      4 ~ \"abr\",\n      5 ~ \"mai\",\n      6 ~ \"jun\",\n      7 ~ \"jul\",\n      8 ~ \"ago\",\n      9 ~ \"set\",\n      10 ~ \"out\",\n      11 ~ \"nov\",\n      12 ~ \"dez\"\n    ),\n    mes = str_to_title(mes),\n    categoria = str_to_title(categoria),\n    mes = factor(mes, ordered = T, levels = c(\n      \"Jan\", \"Fev\", \"Mar\", \"Abr\", \"Mai\",\n      \"Jun\", \"Jul\", \"Ago\", \"Set\", \"Out\", \"Nov\", \"Dez\"\n    ))\n  ) |&gt; \n  mutate( # aqui é onde realizamos a junção mencionadas nos exemplos 1 e 2 \n    inicial = case_when(\n    inicial == \"CHO\" | inicial == \"BOMBOM\" ~ \"CHOC\",\n    inicial == \"SORVETE\" ~ \"SORV\",\n    inicial == \"BISCOITO\" | inicial == \"COOKIE\" | inicial == \"CLUB\" | inicial == \"BARRA\" ~ \"BISC\",\n    inicial == \"DOCE\" | inicial == \"ECLAIR\" | inicial == \"BIBIS\" ~ \"BALA\",\n    inicial == \"CONFEITO\" | inicial == \"MIST\" ~ \"BOLO\",\n    inicial == \"REFR\" | inicial == \"BIPACK\" ~ \"REFRIG\",\n    inicial == \"VH\" ~ \"VHO\",\n    inicial == \"MINI\" ~ \"TOMATE\",\n    .default = as.character(inicial)\n  ))\n\n\n\nGasto total por categoria\nA primeira visualização foi focada em observar o gasto total em cada uma das categorias e o número de compras efetuadas.\n\n# pacotes utilizados em todos os gráficos\nlibrary(tidyverse)\nlibrary(showtext)\nlibrary(ggtext)\n\n\nbase |&gt;\n  group_by(categoria) |&gt;\n  summarise(\n    valor_total = sum(valor_total),\n    numero = n()\n  ) |&gt;\n  ggplot() +\n  aes(x = fct_reorder(categoria, valor_total), y = valor_total) +\n  # geom_col(fill = \"firebrick\") +\n  geom_col(fill = \"firebrick\") +\n  coord_flip() +\n  geom_richtext(\n    mapping = aes(\n      label = \"O nº diz respeito a quantidade &lt;br&gt; de itens comprados\",\n      x = 4, y = 2000\n    ), size = 4,\n    fill = \"gray70\", label.color = NA,\n    family = \"Black Ops One\",\n    label.padding = unit(c(0.30, 0.30, 0.30, 0.30), \"lines\"),\n  ) +\n  labs(\n    x = element_blank(),\n    y = \"Valor total [R$]\",\n    title = \"Gasto total por categoria\",\n    caption = social_caption\n  ) +\n  geom_text(\n    mapping = aes(label = paste(\"nº\", numero)),\n    nudge_y = -125, col = \"gray98\", size = 5, fontface = \"bold\"\n  ) +\n  theme_minimal(23) +\n  theme(\n    text = element_text(family = \"Black Ops One\", size = 20),\n    plot.background = element_rect(fill = \"gray90\"),\n    plot.caption = element_textbox_simple(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nGasto mensal das principais categorias\nO segundo gráfico mostra o gasto mês-a-mês para cada categoria. Para isso utilizamos apenas as 5 maiores categorias em total de gasto.\n\ngg_bonito &lt;- base |&gt;\n  group_by(categoria, mes) |&gt;\n  summarise(\n    valor_total = sum(valor_total),\n    numero = n()\n  ) |&gt;\n  filter(!categoria %in% c(\n    \"Temperos E Molhos\",\n    \"Limpeza\", \"Verduras\",\n    \"Frutas\", \"Higiene\"\n  ))\ngg_bonito2 &lt;- gg_bonito |&gt;\n  group_by(mes) |&gt;\n  mutate(\n    total_mensal = sum(valor_total),\n    valor_cat = total_mensal - valor_total\n  )\n\nggplot(gg_bonito) +\n  aes(\n    x = mes,\n    node = categoria,\n    fill = categoria,\n    value = valor_total\n  ) +\n  geom_sankey_bump(\n    space = 0,\n    type = \"alluvial\",\n    color = \"transparent\",\n    smooth = 6\n  ) +\n  scale_fill_viridis_d(option = \"A\", alpha = .8) +\n  theme_sankey_bump(base_size = 16) +\n  scale_x_discrete(expand = c(.01, .001)) +\n  # coord_cartesian(expand = F, clip = \"off\") +\n  labs(\n    title = \"Despesas mensais em diferentes categorias de alimentos\",\n    y = \"Valor Total Mensal [R$]\",\n    x = element_blank(),\n    fill = NULL,\n    caption = social_caption\n  ) +\n  geom_richtext(\n    mapping = aes(\n      label = \"Espessura da linha indica o  &lt;span style = 'color:tomato;'&gt;gasto da categoria&lt;/span&gt;. &lt;br&gt; O somatório de todas as espessuras &lt;br&gt; indica o &lt;span style = 'color:tomato;'&gt;valor total mensal&lt;/span&gt;.\",\n      x = 3.3, y = 840\n    ), size = 4.4,\n    fill = NA, label.color = NA,\n    family = \"Playfair Display\",\n    label.padding = grid::unit(rep(0, 4), \"pt\")\n  ) +\n  theme(\n    text = element_text(family = \"Playfair Display\", size = 20),\n    legend.position = c(.35, .9),\n    legend.background = element_rect(fill = \"transparent\"),\n    legend.direction = \"horizontal\",\n    legend.text = element_text(size = 15),\n    plot.caption = element_textbox_simple(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nGasto mensal e total dos principais produtos\nNa última análise, consideramos as categorias que registraram o maior número de compras. Para isso, utilizamos as seguintes categorias: carboidratos, doces, bebidas, frutas e verduras. Dentro de cada uma dessas categorias, identificamos os produtos mais frequentemente adquiridos e os representamos em um único gráfico.\nÉ importante mencionar que, embora o tomate seja botanicamente classificado como uma fruta, optamos por incluí-lo na categoria verdura. Colocamos ele na categoria verdura pois comemos na salada e não como sobremesa!\n\ndft &lt;- base |&gt;\n  filter(categoria %in% c(\"Carboidratos\", \"Doces\", \"Bebidas\", \"Frutas\", \"Verduras\")) |&gt;\n  filter(inicial %in% c(\"PAO\", \"CHOC\", \"REFRIG\", \"BANANA\", \"TOMATE\")) |&gt;\n  group_by(mes, categoria, inicial) |&gt;\n  summarise(\n    valor_total = sum(valor_total),\n    numero = n()\n  ) |&gt;\n  mutate(inicial = case_match(\n    inicial,\n    \"BANANA\" ~ \"Banana\",\n    \"CHOC\" ~ \"Chocolate\",\n    \"PAO\" ~ \"Pão\",\n    \"REFRIG\" ~ \"Refrigerante\",\n    \"TOMATE\" ~ \"Tomate\"\n  ))\n\nvalor_dez &lt;- dft |&gt; filter(mes == \"Dez\")\n\ntot_maximo &lt;- dft |&gt;\n  group_by(inicial) |&gt;\n  summarise(total = round(sum(valor_total))) |&gt;\n  mutate(\n    texto = c(\n      \"Sweet Banana\", \"Cream Cheese Demo\",\n      \"Bready Alternates Demo\", \"Loki Cola\",\n      \"Sweet Banana\"\n    ),\n    cor = c(\n      \"gold3\", \"chocolate4\", \"darkgoldenrod3\",\n      \"firebrick3\", \"tomato2\"\n    )\n  ) |&gt;\n  full_join(valor_dez)\n\ngithub_icon &lt;- \"&#xf09b\"\ngit_gf &lt;- \"FGu5tav0\"\ngit_dv &lt;- \"DiogoVBol\"\n\nsocial_caption &lt;- glue::glue(\n  \"&lt;span style='font-family:\\\"Font Awesome 6 Brands\\\";'&gt;{github_icon};&lt;/span&gt;\n  &lt;span style='color: #000000'&gt;{git_gf}&lt;/span&gt; &lt;br&gt; &lt;span style='font-family:\\\"Font Awesome 6 Brands\\\";'&gt;{github_icon};&lt;/span&gt;\n  &lt;span style='color: #000000'&gt;{git_dv}&lt;/span&gt;\"\n)\n\n# com facet ---------------------------------------------------------------\n\ndft |&gt;\n  ggplot() +\n  aes(x = mes, y = valor_total, group = 1) +\n  facet_wrap(~inicial, ncol = 1, scales = \"free_y\") +\n  geom_line(linewidth = 2, col = \"gray40\") +\n  geom_point(mapping = aes(fill = inicial), pch = 21, size = 6, col = \"gray90\", stroke = 2) +\n  labs(\n    x = element_blank(),\n    y = \"Valor Total [R$]\",\n    fill = element_blank(),\n    title = \"Valor mensal pago dos produtos \\nmais consumidos por categoria\",\n    caption = social_caption\n  ) +\n  ggfx::with_shadow(\n    geom_text(\n      data = tot_maximo,\n      mapping = aes(\n        label = inicial,\n        family = texto,\n        x = 3, y = +Inf,\n        col = cor,\n        vjust = 1.4\n      ),\n      nudge_x = 1.3,\n      size = 20,\n      lineheight = 1\n    ),\n    colour = \"gray70\",\n    sigma = 20\n  ) +\n  ylim(0, NA) +\n  scale_x_discrete(expand = expansion(mult = c(0, .20))) +\n  coord_cartesian(clip = \"off\") +\n  geom_text(\n    data = tot_maximo,\n    mapping = aes(\n      label = paste(\"R$\", total),\n      col = cor,\n      x = 12, y = valor_total\n    ),\n    family = \"Banana\",\n    nudge_x = 1.3,\n    size = 10,\n    lineheight = 1\n  ) +\n  scale_color_identity() +\n  scale_fill_manual(values = c(\n    \"gold3\", \"chocolate4\", \"darkgoldenrod3\",\n    \"firebrick3\", \"tomato2\"\n  )) +\n  theme_minimal(28) +\n  theme(\n    strip.text = element_blank(),\n    plot.background = element_rect(fill = \"gray90\"),\n    panel.grid.minor.y = element_blank(),\n    axis.title = element_blank(),\n    axis.text = element_text(family = \"Sweet Banana\"),\n    panel.grid.minor = element_line(),\n    legend.position = \"none\",\n    plot.title = element_text(\n      family = \"Brain Melt regular\",\n      size = 35, hjust = 0\n    ),\n    plot.margin = margin(t = 10, r = 10, b = 10, l = 10),\n    plot.caption = element_textbox_simple(size = 16, hjust = -1, halign = -.05)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nConclusão\nNesse texto, demonstramos como é possível obter informações sobre seus gastos pessoais por meio de web scraping, manipulação e visualização de dados utilizando R e Python. As descobertas aqui foram de caráter exploratório, e uma nova análise com cunho estatístico será realizada em um post futuro. Surgem perguntas interessantes, como: Tenho um dia da semana preferido para ir ao mercado? Ou, a frequência de vezes que vou ao mercado pode prever o preço que gasto no mês?\n\n\n\n\n\n\nNota\n\n\n\nOs códigos apresentados foram produzidos sem critérios de qualidade. Melhorias ainda podem ser feitas.\n\n\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "recap/2024-04-09 artigos doutorandos/index.html",
    "href": "recap/2024-04-09 artigos doutorandos/index.html",
    "title": "Quantos artigos eu preciso ter no currículo até o fim do doutorado em Ciência do Solo?",
    "section": "",
    "text": "A resposta é óbvia: Não sei!\nE não tem como saber, pois, vai depender de perguntas que vão para além do individual de cada um. Agora, não significa que não podemos fazer algumas comparações e tentar sair do escuro. Bem, é exatamente isso que eu trago nesse texto. Não são raros os dias que penso que tudo que faço (da Pós-graduação) leva milhões de anos para ficar pronto e, por consequência, me vejo divagando em uma estrada de pensamentos que não tem fim. Para acalmar minhas expectativas, eu resolvi fazer algo. Gastei parte do meu tempo para tentar encontrar parte da resposta. Assim, investiguei, mesmo que superficialmente, a pergunta: Qual o desempenho dos estudantes de doutorado que já passaram pela pós? Com a esperança de encontrar algo para verificar como está o meu andamento individual. O desempenho é difícil de mensurar e, bem, o que o desempenho realmente mede, né? Mas utilizei o que tinha em meu alcance para traçar uma linha de raciocínio. A informação mais fácil de conseguir de alunos já doutores é o número de artigos que publicaram e que estão no currículo Lattes. Com base nesses números levantei algumas hipóteses: 1. Artigos demandam tempo e o número de publicações reflete o tempo investido. 2. A quantidade de artigos pode indicar as parcerias do aluno. 3. Quanto mais artigos, mais a pessoa se dedica a temas relacionados à pós-graduação. Obviamente, há problemas com essas hipóteses e elas abordam apenas uma parte do problema, mas são as que tenho até o momento. Ainda assim, a necessidade de publicar artigos por parte dos alunos torna essa uma opção interessante a ser considerada.\n\n\nPara minha análise, coletei dados de diversas fontes. A análise foi conduzida exclusivamente com alunos de doutorado que frequentaram o curso de Ciência do Solo entre os anos de 2013 e 2022. O período foi determinado de forma arbitrária, com base no tempo disponível para realizar o trabalho. Como sou aluno da UFRGS, utilizei os dados dos egressos do Programa de Pós-Graduação em Ciência do Solo desta universidade. No entanto, para aumentar a amostra, incluí também dados da pós-graduação em Ciência do Solo da UFSM.\nNos sites de cada programa de pós-graduação (Egressos da UFRGS e Egressos do PPGCS da UFSM), compilei o número e os nomes dos alunos formados em cada um dos anos especificados. Inicialmente, fiquei surpreso ao descobrir que o PPGCS da UFSM possui um levantamento sobre a atuação profissional de seus egressos. Essa é exatamente mais uma das ideias que gostaria de implementar com os alunos do PPGCS da UFRGS, já que desconheço tal informação.\nCom os nomes em mãos, iniciei a tarefa mais trabalhosa de todo o processo: encontrar e baixar o currículo Lattes de cada um deles. Ao todo, foram baixados 86 currículos da UFRGS e 98 da UFSM.\n\n\n\n\n\n\nNota\n\n\n\nOs dados podem ser baixados no meu github link\n\n\nOutra informação importante era determinar os anos em que cada pessoa cursou o doutorado em seus respectivos programas de pós-graduação. Para isso, obtive a base de dados de discentes da CAPES, que abrange o período de 2004 a 2022. Utilizei o currículo Lattes como filtro para identificar os alunos que concluíram o doutorado em cada um dos programas e os anos em que estiveram matriculados.\nA maior dificuldade agora é definir quais artigos cada pessoa publicou durante o doutorado. Inicialmente, não há como fazer essa distinção, pois essa informação não está disponível no Lattes e não posso me dar ao luxo de investir mais tempo nessa investigação. Portanto, estabeleci um critério de corte: considerei apenas os artigos com ano de publicação igual ou anterior ao ano de conclusão do doutorado. Reconheço que, na prática, a maioria dos artigos é publicada no ano seguinte à submissão, e que os alunos tendem a publicar os artigos do doutorado mais próximo do final do curso, o que poderia resultar em anos diferentes. No entanto, essa foi a abordagem que encontrei para manter todos os dados em uma mesma “base”. É importante ressaltar que estamos partindo do zero em termos de informação. Com os resultados obtidos, realizei uma análise descritiva dos dados por meio de histogramas. Além disso, elaborei um gráfico da frequência total de artigos publicados e conduzi uma análise dos títulos dos artigos por meio de uma nuvem de palavras, levando em consideração as palavras mais frequentes encontradas nos artigos. Por fim, realizei uma análise da frequência com que termos como nitrogênio, fósforo e potássio aparecem nos títulos, como uma forma indireta de compreender a tendência ou determinar qual é considerado mais importante.\n\n\n\nDos 184 alunos da amostra, um total de 1517 artigos foram encontrados, porém 4 dos alunos não tinham nenhum artigo em seus currículos Lattes. Isso suscita uma discussão inicial sobre a importância do Lattes e a necessidade de mantê-lo atualizado. O Lattes é um exemplo mundial de uma plataforma bem-sucedida que armazena dados de pesquisadores e alunos. Apesar de suas falhas, é inegável seu valor como um repositório de acesso público. De fato, o Lattes é a fonte de dados para várias pesquisas interessantes. O histograma (Figura 1) mostra que a distribuição de artigos publicados com ano igual ou inferior ao ano de conclusão do doutorado é assimétrica negativa ou se aproxima disso. Com um número maior de pessoas publicando números pequenos de artigos frente a números mais altos.\n Figura 1: Histograma da distribuição do número de artigos por aluno.\n\n\n\n\n\n\nAviso\n\n\n\num modelo de massa de probabilidade será ajustado no futuro\n\n\nA Figura 2 apresenta os dados expressos em termos de frequência (absoluta e acumulada). O número médio de artigos foi de 8; no entanto, devido à natureza assimétrica da curva, a média pode não representar adequadamente os dados. Portanto, a mediana, com um valor de 6 artigos, pode ser mais relevante. Isso revela uma descoberta significativa: o número mais frequente corresponde à própria mediana. Além disso, mais de 50% dos alunos no banco de dados publicaram entre 2 e 8 artigos. A avaliação se isso é muito ou pouco depende de vários aspectos. Outro dado interessante é que 90% dos alunos tiveram entre 0 e 18 artigos. No entanto, ao observar o gráfico, é evidente que números elevados são raros. Espero que esta breve análise auxilie meus colegas de mestrado e doutorado em Ciência do Solo a refletirem sobre sua produção acadêmica.\n Figura 2: Frequência relativa e acumuada do número de artigos por aluno de doutorado.\n\n\nCom os títulos, temos a nuvem de palavras. Essa visualização mostra as palavras mais frequentes nos títulos dos mais de mil artigos avaliados. Como esperado, ‘Solo’ e ‘Brasil’ são as palavras mais comuns. Em seguida, encontramos palavras como ‘produtividade’, ‘diferentes’, ‘nitrogênio’, ‘fósforo’ e assim por diante. Acredito que uma mensagem interessante aqui seja tentar evitar palavras óbvias. Embora seja desafiador, aumentar a diversidade nos títulos é uma prática enriquecedora.\n\n\n\n\n\n\n\nBem, como estou realizando meu doutorado com potássio, achei interessante verificar a frequência com que ele é mencionado nos títulos dos artigos. Surpreendentemente, ele é pouco mencionado. A Figura 3 mostra uma visualização do número de vezes que N, P e K aparecem nos títulos, tanto em português quanto em inglês. É interessante notar a disparidade significativa entre os elementos. Não sei se isso está relacionado à importância, à dificuldade de pesquisa ou a algum outro fator que possa explicar essa diferença. Acredito que ainda não tenhamos descoberto tudo o que podemos sobre nenhum desses elementos.\n Figura 2: Número de vezes que N, P e K foram encontrados nos títulos dos artigos.\n\n\n\n\nPara concluir, determinar o número ideal de artigos é uma tarefa complexa, e esta análise está longe de fornecer uma resposta definitiva. No entanto, é inegável que os dados coletados oferecem informações importantes. Espero que essas descobertas incentivem meus colegas a refletirem sobre suas próprias jornadas acadêmicas. Questões como essa são fascinantes de investigar, e caso surjam dúvidas ou alguém queira contribuir, não hesite em enviar uma mensagem."
  },
  {
    "objectID": "recap/2024-04-09 artigos doutorandos/index.html#metodologia",
    "href": "recap/2024-04-09 artigos doutorandos/index.html#metodologia",
    "title": "Quantos artigos eu preciso ter no currículo até o fim do doutorado em Ciência do Solo?",
    "section": "",
    "text": "Para minha análise, coletei dados de diversas fontes. A análise foi conduzida exclusivamente com alunos de doutorado que frequentaram o curso de Ciência do Solo entre os anos de 2013 e 2022. O período foi determinado de forma arbitrária, com base no tempo disponível para realizar o trabalho. Como sou aluno da UFRGS, utilizei os dados dos egressos do Programa de Pós-Graduação em Ciência do Solo desta universidade. No entanto, para aumentar a amostra, incluí também dados da pós-graduação em Ciência do Solo da UFSM.\nNos sites de cada programa de pós-graduação (Egressos da UFRGS e Egressos do PPGCS da UFSM), compilei o número e os nomes dos alunos formados em cada um dos anos especificados. Inicialmente, fiquei surpreso ao descobrir que o PPGCS da UFSM possui um levantamento sobre a atuação profissional de seus egressos. Essa é exatamente mais uma das ideias que gostaria de implementar com os alunos do PPGCS da UFRGS, já que desconheço tal informação.\nCom os nomes em mãos, iniciei a tarefa mais trabalhosa de todo o processo: encontrar e baixar o currículo Lattes de cada um deles. Ao todo, foram baixados 86 currículos da UFRGS e 98 da UFSM.\n\n\n\n\n\n\nNota\n\n\n\nOs dados podem ser baixados no meu github link\n\n\nOutra informação importante era determinar os anos em que cada pessoa cursou o doutorado em seus respectivos programas de pós-graduação. Para isso, obtive a base de dados de discentes da CAPES, que abrange o período de 2004 a 2022. Utilizei o currículo Lattes como filtro para identificar os alunos que concluíram o doutorado em cada um dos programas e os anos em que estiveram matriculados.\nA maior dificuldade agora é definir quais artigos cada pessoa publicou durante o doutorado. Inicialmente, não há como fazer essa distinção, pois essa informação não está disponível no Lattes e não posso me dar ao luxo de investir mais tempo nessa investigação. Portanto, estabeleci um critério de corte: considerei apenas os artigos com ano de publicação igual ou anterior ao ano de conclusão do doutorado. Reconheço que, na prática, a maioria dos artigos é publicada no ano seguinte à submissão, e que os alunos tendem a publicar os artigos do doutorado mais próximo do final do curso, o que poderia resultar em anos diferentes. No entanto, essa foi a abordagem que encontrei para manter todos os dados em uma mesma “base”. É importante ressaltar que estamos partindo do zero em termos de informação. Com os resultados obtidos, realizei uma análise descritiva dos dados por meio de histogramas. Além disso, elaborei um gráfico da frequência total de artigos publicados e conduzi uma análise dos títulos dos artigos por meio de uma nuvem de palavras, levando em consideração as palavras mais frequentes encontradas nos artigos. Por fim, realizei uma análise da frequência com que termos como nitrogênio, fósforo e potássio aparecem nos títulos, como uma forma indireta de compreender a tendência ou determinar qual é considerado mais importante."
  },
  {
    "objectID": "recap/2024-04-09 artigos doutorandos/index.html#resultados",
    "href": "recap/2024-04-09 artigos doutorandos/index.html#resultados",
    "title": "Quantos artigos eu preciso ter no currículo até o fim do doutorado em Ciência do Solo?",
    "section": "",
    "text": "Dos 184 alunos da amostra, um total de 1517 artigos foram encontrados, porém 4 dos alunos não tinham nenhum artigo em seus currículos Lattes. Isso suscita uma discussão inicial sobre a importância do Lattes e a necessidade de mantê-lo atualizado. O Lattes é um exemplo mundial de uma plataforma bem-sucedida que armazena dados de pesquisadores e alunos. Apesar de suas falhas, é inegável seu valor como um repositório de acesso público. De fato, o Lattes é a fonte de dados para várias pesquisas interessantes. O histograma (Figura 1) mostra que a distribuição de artigos publicados com ano igual ou inferior ao ano de conclusão do doutorado é assimétrica negativa ou se aproxima disso. Com um número maior de pessoas publicando números pequenos de artigos frente a números mais altos.\n Figura 1: Histograma da distribuição do número de artigos por aluno.\n\n\n\n\n\n\nAviso\n\n\n\num modelo de massa de probabilidade será ajustado no futuro\n\n\nA Figura 2 apresenta os dados expressos em termos de frequência (absoluta e acumulada). O número médio de artigos foi de 8; no entanto, devido à natureza assimétrica da curva, a média pode não representar adequadamente os dados. Portanto, a mediana, com um valor de 6 artigos, pode ser mais relevante. Isso revela uma descoberta significativa: o número mais frequente corresponde à própria mediana. Além disso, mais de 50% dos alunos no banco de dados publicaram entre 2 e 8 artigos. A avaliação se isso é muito ou pouco depende de vários aspectos. Outro dado interessante é que 90% dos alunos tiveram entre 0 e 18 artigos. No entanto, ao observar o gráfico, é evidente que números elevados são raros. Espero que esta breve análise auxilie meus colegas de mestrado e doutorado em Ciência do Solo a refletirem sobre sua produção acadêmica.\n Figura 2: Frequência relativa e acumuada do número de artigos por aluno de doutorado.\n\n\nCom os títulos, temos a nuvem de palavras. Essa visualização mostra as palavras mais frequentes nos títulos dos mais de mil artigos avaliados. Como esperado, ‘Solo’ e ‘Brasil’ são as palavras mais comuns. Em seguida, encontramos palavras como ‘produtividade’, ‘diferentes’, ‘nitrogênio’, ‘fósforo’ e assim por diante. Acredito que uma mensagem interessante aqui seja tentar evitar palavras óbvias. Embora seja desafiador, aumentar a diversidade nos títulos é uma prática enriquecedora.\n\n\n\n\n\n\n\nBem, como estou realizando meu doutorado com potássio, achei interessante verificar a frequência com que ele é mencionado nos títulos dos artigos. Surpreendentemente, ele é pouco mencionado. A Figura 3 mostra uma visualização do número de vezes que N, P e K aparecem nos títulos, tanto em português quanto em inglês. É interessante notar a disparidade significativa entre os elementos. Não sei se isso está relacionado à importância, à dificuldade de pesquisa ou a algum outro fator que possa explicar essa diferença. Acredito que ainda não tenhamos descoberto tudo o que podemos sobre nenhum desses elementos.\n Figura 2: Número de vezes que N, P e K foram encontrados nos títulos dos artigos."
  },
  {
    "objectID": "recap/2024-04-09 artigos doutorandos/index.html#conclusão",
    "href": "recap/2024-04-09 artigos doutorandos/index.html#conclusão",
    "title": "Quantos artigos eu preciso ter no currículo até o fim do doutorado em Ciência do Solo?",
    "section": "",
    "text": "Para concluir, determinar o número ideal de artigos é uma tarefa complexa, e esta análise está longe de fornecer uma resposta definitiva. No entanto, é inegável que os dados coletados oferecem informações importantes. Espero que essas descobertas incentivem meus colegas a refletirem sobre suas próprias jornadas acadêmicas. Questões como essa são fascinantes de investigar, e caso surjam dúvidas ou alguém queira contribuir, não hesite em enviar uma mensagem."
  },
  {
    "objectID": "formacao.html",
    "href": "formacao.html",
    "title": "Formação",
    "section": "",
    "text": "Educação\n Atualmente, doutorando em ciência do solo pela Universidade Federal do Rio Grande do Sul  (2022 - ).\n Mestre em ciência do solo pela Universidade Federal do Rio Grande do Sul (UFRGS) (2020 - 2022).\n Engenheiro agrônomo pelo Instituto Federal do Paraná  (2015 - 2019).\n Técnico em Agropecuária pelo Instituto Federal Catarinense  (2012 - 2014).\n\n\nInteresses\n\n\n\n\n\nCiência do Solo\nAnálise de dados\n\n\n\n\nMineralogia\nManipulação\n\n\nQuímica\nInferência\n\n\nFertilidade\nVisualização\n\n\n\n\n\n\n\n\n\nHabilidades\n     R\n     Markdown/Quarto\n     Visualização/Manipulação de dados\n     Modelos\n     Python\n\n\n\n\n De volta ao topo"
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#education",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#education",
    "title": "Finley Malloc",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#experience",
    "href": "projetos/2023-11-01 disciplina de R para ufrgs solos/index.html#experience",
    "title": "Finley Malloc",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#como-ela-se-ajusta",
    "href": "recap/2024-08-01 regressão linear/index.html#como-ela-se-ajusta",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "2.1 Como ela se ajusta?",
    "text": "2.1 Como ela se ajusta?\nPara encontrar a curva com o melhor ajuste, é utilizado o método dos mínimos quadrados. Isso significa que o método “encontra” qual das curvas retorna o menor valor para a soma dos quadrados. Essa soma dos quadrados é obtida calculando a diferença entre o valor observado e o valor predito, elevando essa diferença ao quadrado e, somando tudo.\nEm outras palavras, esse método encontra “a curva que fica mais próxima dos dados observados”. É claro que a quantidade de matemática e estatística por trás de todo esse método é brutal. Aqui apresento apenas uma explicação superficial.\nA Figura 2 mostra uma curva (\\(a\\)) com inclinação zero, que representa o intercepto na média dos valores de \\(y\\). Já em \\(b\\), temos o melhor ajuste possível para uma regressão linear. A primeira curva pode ser considerada um palpite inicial para explicar os dados, ou seja, utilizando o valor da média. A segunda é um avanço, onde \\(y\\) passa a assumir valores dependendo de \\(x\\).\n\n\n\n\n\n\n\n\nFigura 2: Ajsute de curva para média de y (a) e ajsute de menor erro (b)\n\n\n\n\n\nA animação abaixo mostra a reta de regressão assumindo diversos valores para \\(\\beta_{0}\\) e \\(\\beta_{1}\\). É importante observar que ela começa com uma reta na média de \\(y\\) e evolui até alcançar retas para além da mais adequada. Os valores da soma dos quadrados indicam que há uma inclinação que minimiza esse valor, enquanto inclinações anteriores ou posteriores fazem com que a soma dos quadrados aumente."
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#variável-categórica-na-regressão",
    "href": "recap/2024-08-01 regressão linear/index.html#variável-categórica-na-regressão",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "2.2 Variável categórica na regressão",
    "text": "2.2 Variável categórica na regressão\nEssa é a parte que mais me chama atenção.\nPara que uma variável categórica entre na análise de regressão, ela passa por uma pequena transformação, tornando-se uma variável dummy.\nPor definição, uma variável dummy assume apenas os valores \\(0\\) ou \\(1\\), indicando a presença ou ausência de uma categoria. O número de variáveis dummy criadas é sempre igual a \\(n-1\\), onde \\(n\\) é o número de categorias da variável.\nSe tivermos \\(3\\) grupos em nossa variável, então teremos \\(2\\) variáveis dummy, como mostrado na Tabela 1. Nota-se que um dos grupos sempre terá o valor \\(0\\) em ambas as dummies, pois serve como referência, e os demais grupos serão comparados a ele. Na Tabela 1, o grupo “A” é a referência inicial, mas para as demais comparações, é possível alterar o grupo de referência.\n\n\n\n\nTabela 1: Representação das variáveis dummy\n\n\n\n\n\n\nGrupo\nDummy (D1)\nDummy (D2)\n\n\n\n\nA (ref)\n0\n0\n\n\nB\n1\n0\n\n\nC\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDummy em programas estatísticos\n\n\n\n\n\nA criação das variáveis dummy é totalmente automatizada nos programas estatísticos. Logo, você não precisa criá-las manualmente. No entanto, como exercicio, vale a pena rodar uma regressão criando as próprias dummies e ver que o resultado é exatamente o mesmo que o do computador.\n\n\n\nA equação de regressão geral pode ser descrita conforme Equação 1. Também podemos fazer uma equação para cada grupo (Equação 2, Equação 3 e Equação 4).\n\\[y=\\beta_{0}+\\beta_{1}D1+\\beta_{2}D2+\\epsilon_{i} \\tag{1}\\]\nEquação grupo A:\n\\[y=\\beta_{0}+\\epsilon_{i} \\tag{2}\\]\nEquação grupo B:\n\\[y=\\beta_{0}+\\beta_{1}D1+\\epsilon \\tag{3}\\]\nEquação grupo C:\n\\[y=\\beta_{0}+\\beta_{2}D2+\\epsilon \\tag{4}\\]\nAgora, repare como a Equação 2 é descrita apenas pelo intercepto, pois a variável de grupo assume o valor \\(0\\) em todas as dummies (Tabela 1). Já para os demais grupos aparece na equaçao as dummies em que o valor \\(1\\) é atribuído.\nA interpretação de uma regressão com variáveis dummy é, basicamente, a diferença de média entre os grupos (semelhante a um teste de Tukey). Na Figura 3, podemos ver as médias dos grupos A, B e C e os respectivos \\(\\beta\\)s, que indicam a diferença entre elas. Essa diferença pode ser entendida como uma diferença angular, pois, se as médias são diferentes, forma-se um ângulo (\\(\\neq 0\\)) entre as médias.\n\n\n\n\n\n\n\n\nFigura 3: Representação esquemática da regressão com variável dummy"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#requisitosindicadores-de-qualidade-para-a-regressão-linear",
    "href": "recap/2024-08-01 regressão linear/index.html#requisitosindicadores-de-qualidade-para-a-regressão-linear",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "2.3 Requisitos/indicadores de qualidade para a regressão linear",
    "text": "2.3 Requisitos/indicadores de qualidade para a regressão linear\nComo um professor disse uma vez:\n\nO grande perigo é a pessoa apenas saber fazer uma regressão.\n\nEssa frase é uma crítica às pessoas que apenas sabem montar um modelo e sair interpretando. Uma interpretação útil surge de uma regressão bem feita, e, para isso, é necessário conhecer a qualidade do modelo gerado. Abaixo estão os principais critérios para avaliar como seu modelo se comporta.\n\n2.3.1 As observações devem ser independentes\nEsse requisito diz que o valor de uma observação não pode ter influência no valor de outra observação.\n\n\n2.3.2 As variáveis preditivas não devem possuir alta correlação\nTambém chamada de multicolinearidade, a alta correlação entre as variáveis independentes é um aspecto importante a ser considerado. Variáveis com alta correlação podem causar diversos problemas, como distorção dos resultados e interpretações espúrias.\n\n\n2.3.3 Homocedasticidade e normalidade dos resíduos\nA homocedasticidade é semelhante à homogeneidade de variâncias testada na ANOVA. No entanto, a homocedasticidade diz respeito à variância dos erros do modelo (\\(\\epsilon\\)), que deve ser constante em toda a escala ou nos níveis da variável \\(x\\).\nJá a normalidade diz respeito aos erros do modelo, que devem ser aproximadamente normais."
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#especificando-o-modelo",
    "href": "recap/2024-08-01 regressão linear/index.html#especificando-o-modelo",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "3.1 Especificando o modelo",
    "text": "3.1 Especificando o modelo\nNosso modelo teórico e sem interação pode ser representado como:\n\\[y= \\beta_0 + \\beta_1*dose + \\beta_2*cultivar + \\epsilon\\]"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#resultado",
    "href": "recap/2024-08-01 regressão linear/index.html#resultado",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "3.2 Resultado",
    "text": "3.2 Resultado\nA Tabela 3 mostra os resultados da regressão. Na tabela, a primeira coluna representa o intercepto ou a variável \\(x\\). A coluna “Beta” apresenta os coeficientes ajustados, seguida pelo intervalo de confiança para o coeficiente e, por fim, o valor de p.\n\n\n\n\nTabela 3: Resultado da regressão linear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n49.9\n48.2, 51.6\n&lt;0.001\n\n\nCultivar\n\n\n\n\n\n\n\n\n    Cultivar1\n—\n—\n\n\n\n\n    Cultivar2\n-4.44\n-6.30, -2.58\n&lt;0.001\n\n\n    Cultivar3\n-11.4\n-13.3, -9.58\n&lt;0.001\n\n\nDose\n0.103\n0.092, 0.113\n&lt;0.001\n\n\nNo. Obs.\n150\n\n\n\n\n\n\nR²\n0.775\n\n\n\n\n\n\n\n1\nCI = Confidence Interval"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#interpretação",
    "href": "recap/2024-08-01 regressão linear/index.html#interpretação",
    "title": "Do caos ao conhecimento:  use y = β0 + β1x + ϵ para explicar seus experimentos",
    "section": "Interpretação",
    "text": "Interpretação\nO modelo criado (Tabela 4) tem um \\(R^2\\) de \\(0,77\\), o que é considerao um valor “bom”. Todas as variáveis \\(x\\) foram significativas (\\(p&lt;0,05\\)). A interpretação de cada variável é:\nCultivar: A Cultivar2 tem uma produtividade em média de 4,44 unidade a menos que a Cultivar1. Já a Cultivar3 tem produtividade média de 11,4 unidades a menos que a Cultivar1. Alterando a referência para a Cultivar2, temos que a Cultivar3 tem produtividade média de 7 unidade a menos que a Cultivar2\nDose: Com relação a dose, a cada aumento de uma unidade temos um aumento na produtividade de 0,103 unidades.\n\n\n\n\nTabela 4: Resultado da regressão linear entre Dose e Cultivar para prever produtividade. “Refe Cultivar1” são os resultados utilizando a categória “Cultivar1” como referência, já “Refe Cultivar2” é utilizada a “Cultivar2”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nRefe Cultivar1\n\n\nRefe Cultivar2\n\n\n\nBeta\n95% CI\n1\np-value\nBeta\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n49.9\n48.2, 51.6\n&lt;0.001\n45.5\n43.8, 47.2\n&lt;0.001\n\n\nCultivar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Cultivar1\n—\n—\n\n\n4.44\n2.58, 6.30\n&lt;0.001\n\n\n    Cultivar2\n-4.44\n-6.30, -2.58\n&lt;0.001\n—\n—\n\n\n\n\n    Cultivar3\n-11.4\n-13.3, -9.58\n&lt;0.001\n-7.00\n-8.87, -5.14\n&lt;0.001\n\n\nDose\n0.103\n0.092, 0.113\n&lt;0.001\n0.103\n0.092, 0.113\n&lt;0.001\n\n\nNo. Obs.\n150\n\n\n\n\n\n\n\n\n\n\n\n\nR²\n0.775\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nCI = Confidence Interval"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#indicadores-de-qualidade",
    "href": "recap/2024-08-01 regressão linear/index.html#indicadores-de-qualidade",
    "title": "Do caos ao conhecimento:  use y = β0 + β1x + ϵ para explicar seus experimentos",
    "section": "Indicadores de qualidade",
    "text": "Indicadores de qualidade\n\nOutliers\nNa regressão linear, uma das maneira de verificar se há dados influentes para o modelo é por meio da distância de cook3. Existem diferentes interpretações frente a valores de corte para considerar um valor influente. Aqui, utilizo o valor de \\(1\\) como ponto de corte. Há discussões sobre o uso de valores limite para diagnosticos, mas de qualquer forma antes de nada, utilizar o limite de \\(1\\) já é grandiosso.\nPodemos observar na Figura 4 que nenhum ponto chega perto do valor de \\(1\\), portanto aparentemente não temos pontos discrepantes que podem atrapalhar o modelo.\n\n\n\n\n\n\n\n\nFigura 4: Distância de cook para o modelo de regressão\n\n\n\n\n\n\n\nNormalidade dos resíduos\nO Figura 5 mostra o qq-plot dos resíduos. O ideal é que os pontos estejam dispostos perfeitamente em cima da reta, no entando isso é práticamente impossível nas análises do dia-a-dia. Portanto, quanto mais próximo da linha melhor e não temos um ponto de corte ou metrica de qualidade para qq-plot buscamos o qq-plot melhor dados os dados que temos.\nVerificamos que há alguns escapes da linha, mas no geral está ok. Com isso podemos concluir que os resíduos são aproximadamente normais. Além disso, tudo indica que não temos problemas com a homocedasticidade.\n\n\n\n\n\n\n\n\nFigura 5: QQ-plot dos resíduos do modelo\n\n\n\n\n\n\n\nMulticolinearidade\nA tolerância entra as variáveis foi de \\(1\\) (Tabela 5) o que significa uma boa tolerância. Em geral, acima de \\(0,80\\) já temos variáveis que se toleram no modelo.\n\n\n\n\nTabela 5: QQ-plot dos resíduos do modelo\n\n\n\n\n\n\nTerm\nVIF\nTolerance\n\n\n\n\nCultivar\n1\n1\n\n\nDose\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nz\np\nMethod\nAlternative\n\n\n\n\n2.08806\n0.61766\nDurbin-Watson test\ntrue autocorrelation is greater than 0\n\n\n\n\n\ntolerância: aceitável é a partir de 0.8\nsaídas: remover uma das variáveis, juntar as duas em um fator (um pca), ridge regression\n\n\nHomocedasticidade\nNormalidade (semetria) dos resíduos em todos os níveis da VD (Y)\nHomogeneidade da variância dos resíduos do modelo\nqq-plot do resíduo"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#footnotes",
    "href": "recap/2024-08-01 regressão linear/index.html#footnotes",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "Notas de rodapé",
    "text": "Notas de rodapé\n\n\nFrancis Galton↩︎\nwikipedia↩︎\nDennis Cook↩︎"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#sec-interpretacao",
    "href": "recap/2024-08-01 regressão linear/index.html#sec-interpretacao",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "3.3 Interpretação",
    "text": "3.3 Interpretação\nO modelo criado (Tabela 4) tem um \\(R^2\\) de \\(0,77\\), o que é considerado um valor “bom”. Todas as variáveis \\(x\\) foram significativas (\\(p&lt;0,05\\)). A interpretação de cada variável é:\nCultivar: A Cultivar2 tem uma produtividade em média de 4,44 unidades a menos que a Cultivar1. Já a Cultivar3 tem produtividade média de 11,4 unidades a menos que a Cultivar1. Alterando a referência para a Cultivar2, observa-se que a Cultivar3 tem produtividade média de 7 unidades a menos que a Cultivar2\nDose: Com relação a dose, a cada aumento de uma unidade temos um aumento na produtividade de 0,103 unidades.\n\n\n\n\nTabela 4: Resultado da regressão linear entre Dose e Cultivar para prever produtividade. “Refe Cultivar1” são os resultados utilizando a categória “Cultivar1” como referência, já “Refe Cultivar2” é utilizada a “Cultivar2”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nRefe Cultivar1\n\n\nRefe Cultivar2\n\n\n\nBeta\n95% CI\n1\np-value\nBeta\n95% CI\n1\np-value\n\n\n\n\n(Intercept)\n49.9\n48.2, 51.6\n&lt;0.001\n45.5\n43.8, 47.2\n&lt;0.001\n\n\nCultivar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Cultivar1\n—\n—\n\n\n4.44\n2.58, 6.30\n&lt;0.001\n\n\n    Cultivar2\n-4.44\n-6.30, -2.58\n&lt;0.001\n—\n—\n\n\n\n\n    Cultivar3\n-11.4\n-13.3, -9.58\n&lt;0.001\n-7.00\n-8.87, -5.14\n&lt;0.001\n\n\nDose\n0.103\n0.092, 0.113\n&lt;0.001\n0.103\n0.092, 0.113\n&lt;0.001\n\n\nNo. Obs.\n150\n\n\n\n\n\n\n\n\n\n\n\n\nR²\n0.775\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nCI = Confidence Interval"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#sec-qualidade",
    "href": "recap/2024-08-01 regressão linear/index.html#sec-qualidade",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "3.4 Indicadores de qualidade",
    "text": "3.4 Indicadores de qualidade\n\n3.4.1 Outliers\nNa regressão linear, uma das maneiras de verificar se há dados influentes para o modelo é por meio da distância de Cook. Existem diferentes interpretações em relação aos valores de corte para considerar um valor influente. Aqui, utilizo o valor de \\(1\\) como ponto de corte. Há discussões sobre o uso de valores limite para diagnósticos, mas, de qualquer forma, utilizar o limite de \\(1\\) já é melhor que não fazer uma avaliação de outliers.\nPodemos observar na Figura 4 que nenhum ponto se aproxima do valor de \\(1\\), portanto, aparentemente, não temos pontos discrepantes que possam prejudicar o modelo.\n\n\n\n\n\n\n\n\nFigura 4: Distância de cook para o modelo de regressão\n\n\n\n\n\n\n\n3.4.2 Normalidade dos resíduos e homocedasticidade\nA Figura 5 mostra o QQ-plot dos resíduos. O ideal é que os pontos estejam dispostos perfeitamente em cima da reta, mas isso é praticamente impossível nas análises do dia a dia. Portanto, quanto mais próximos da linha, melhor. Não temos um ponto de corte ou métrica de qualidade específicos para o QQ-plot. Buscamos o melhor ajuste possível, dados os dados que temos.\nVerificamos que há alguns desvios em relação à linha, mas, no geral, o resultado é satisfatório. Com isso, podemos concluir que os resíduos são aproximadamente normais. Além disso, tudo indica que não temos problemas com a homocedasticidade.\n\n\n\n\n\n\n\n\nFigura 5: QQ-plot dos resíduos do modelo\n\n\n\n\n\n\n\n3.4.3 Multicolinearidade\nA tolerância entre as variáveis foi de \\(1\\) (Tabela 5) o que significa uma boa tolerância. Em geral, acima de \\(0,80\\) já temos variáveis que se toleram no modelo.\n\n\n\n\nTabela 5: Tolerância entre as variáveis do modelo\n\n\n\n\n\n\nTerm\nVIF\nTolerance\n\n\n\n\nCultivar\n1\n1\n\n\nDose\n1\n1\n\n\n\n\n\n\n\n\nO teste de Durbin-Watson é utilizado para verificar a autocorrelação entre os resíduos da regressão. O valor ideal desse teste é \\(2\\), o que indica a ausência de autocorrelação. Se o valor for maior ou menor que \\(2\\), há indícios de autocorrelação positiva ou negativa, respectivamente.\nA análise do Durbin-Watson (Tabela 6) mostra um valor de \\(2,08\\), ou seja, é aproximadamente \\(2\\) e indica que não temos autocorrelação.\n\n\n\n\nTabela 6: QQ-plot dos resíduos do modelo\n\n\n\n\n\n\nz\np\nMethod\nAlternative\n\n\n\n\n2.08806\n0.61766\nDurbin-Watson test\ntrue autocorrelation is greater than 0"
  },
  {
    "objectID": "recap/2024-08-01 regressão linear/index.html#representação-gráfica-do-modelo",
    "href": "recap/2024-08-01 regressão linear/index.html#representação-gráfica-do-modelo",
    "title": "Do caos ao conhecimento:  use \\(y = \\beta_{0} + \\beta_{1}x + \\epsilon\\) para explicar seus experimentos",
    "section": "3.5 Representação gráfica do modelo",
    "text": "3.5 Representação gráfica do modelo\nNa Seção 3.3 vimos quais foram os efeitos significativos do modelo, já na Seção 3.4 observamos que o modelo está adequado e explica bem os dados. Agora, criamos uma visualização gráfica para apresentar melhor os resultados. Poderia ter sido feito um gráfico com as médias dos grupos de cultivares, mostrando o intervalo de confiança e uma regressão com as doses, mas preferi representar dessa forma."
  }
]